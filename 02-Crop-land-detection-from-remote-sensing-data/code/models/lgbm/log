
starting lgbm hyperparameter tuning...
2021-12-17 15:40:46.182287
{'max_depth': 18, 'num_leaves': 143, 'feature_fraction': 0.5398760642626285, 'bagging_fraction': 0.9304436544614162, 'learning_rate': 0.06525287721325376, 'max_bin': 24, 'min_data_in_leaf': 20, 'subsample': 0.175744924178873}
lgbm hyperparameter tuning ends...
2021-12-17 15:43:18.858649

2021-12-17 15:47:03.254172
[LightGBM] [Warning] feature_fraction is set=0.5398760642626285, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5398760642626285
[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20
[LightGBM] [Warning] bagging_fraction is set=0.9304436544614162, subsample=0.175744924178873 will be ignored. Current value: bagging_fraction=0.9304436544614162
lgbm  f1_score on training dataset:
0.9417398031317523
              precision    recall  f1-score   support

         0.0       0.90      0.86      0.88     18709
         1.0       0.93      0.95      0.94     37091

    accuracy                           0.92     55800
   macro avg       0.92      0.91      0.91     55800
weighted avg       0.92      0.92      0.92     55800

[0 0 1 ... 1 1 0]
              precision    recall  f1-score   support

         0.0       0.82      0.75      0.78      2074
         1.0       0.88      0.91      0.90      4126

    accuracy                           0.86      6200
   macro avg       0.85      0.83      0.84      6200
weighted avg       0.86      0.86      0.86      6200


Process finished with exit code 0
